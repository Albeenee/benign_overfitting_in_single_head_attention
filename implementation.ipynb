{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benign Overfitting in Single-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_samples = 200\n",
    "d = 40000\n",
    "rho = 30\n",
    "eta = 0.05\n",
    "beta = 0.025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_samples, d, rho, eta=0.0):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - n_samples: Number of data points to generate.\n",
    "    - d: Dimensionality of the feature space.\n",
    "    - rho: Norm of the signal vectors (mu1 and mu2).\n",
    "    - eta: Label-flipping probability (default: 0.0).\n",
    "\n",
    "    Returns:\n",
    "    - X: Generated data points, shape (n_samples, 2, d).\n",
    "    - y: Labels, shape (n_samples,).\n",
    "    - clean_1: Indices of non-flipped samples where u = mu1.\n",
    "    - clean_2: Indices of non-flipped samples where u = mu2.\n",
    "    - noisy_1: Indices of flipped samples where u = mu1.\n",
    "    - noisy_2: Indices of flipped samples where u = mu2.\n",
    "    \"\"\"\n",
    "    print('Generating data...')\n",
    "\n",
    "    # Generate orthogonal signal vectors mu1 and mu2\n",
    "    mu1 = np.random.randn(d)\n",
    "    mu1 = mu1 / np.linalg.norm(mu1) * rho\n",
    "\n",
    "    mu2 = np.random.randn(d)\n",
    "    mu2 -= mu2 @ mu1 / (np.linalg.norm(mu1)**2) * mu1  # Orthogonalize mu2 w.r.t mu1\n",
    "    mu2 = mu2 / np.linalg.norm(mu2) * rho\n",
    "\n",
    "    # Generate labels\n",
    "    y = np.random.choice([+1, -1], size=n_samples)\n",
    "\n",
    "    # Precompute noise covariance matrix\n",
    "    covariance_matrix = np.eye(d) - (np.outer(mu1, mu1) + np.outer(mu2, mu2)) / (rho**2)\n",
    "\n",
    "    # Generate noise vectors\n",
    "    noise = np.random.multivariate_normal(mean=np.zeros(d), cov=covariance_matrix, size=n_samples)\n",
    "\n",
    "    # Generate signals\n",
    "    signals = np.where(y[:, None] == 1, mu1, mu2)  # Shape: (n_samples, d)\n",
    "\n",
    "    # Create the data points\n",
    "    X = np.zeros((n_samples, 2, d))\n",
    "    token_choices = np.random.choice([0, 1], size=n_samples)  # Randomly select which token gets the signal\n",
    "    X[np.arange(n_samples), token_choices, :] = signals\n",
    "    X[np.arange(n_samples), 1 - token_choices, :] = noise\n",
    "\n",
    "    # Apply label flipping\n",
    "    flip_mask = np.random.rand(n_samples) < eta\n",
    "    y_flipped = np.where(flip_mask, -y, y)\n",
    "\n",
    "    # Separate indices for clean and noisy data\n",
    "    clean_indices = np.where(~flip_mask)[0]\n",
    "    noisy_indices = np.where(flip_mask)[0]\n",
    "\n",
    "    '''clean_1 = clean_indices[y[clean_indices] == 1]\n",
    "    clean_2 = clean_indices[y[clean_indices] == -1]\n",
    "    noisy_1 = noisy_indices[y[noisy_indices] == 1]\n",
    "    noisy_2 = noisy_indices[y[noisy_indices] == -1]'''\n",
    "\n",
    "    print('Data generated.')\n",
    "    return X, y_flipped\n",
    "#clean_1, clean_2, noisy_1, noisy_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "X, y  = generate_data(n_samples, d, rho, eta)\n",
    "num_iterations = 2\n",
    "beta = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-head Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_head_attention(X, p, v):\n",
    "    \"\"\"\n",
    "    Compute the single-head attention model.\n",
    "\n",
    "    Parameters:\n",
    "    - X: numpy array of shape (n_samples, 2, d), input data where each sample has two tokens.\n",
    "    - p: numpy array of shape (d,), trainable attention vector.\n",
    "    - v: numpy array of shape (d,), trainable linear head vector.\n",
    "\n",
    "    Returns:\n",
    "    - f_values: numpy array of shape (n_samples,), attention model outputs for each sample.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute attention scores using the softmax function\n",
    "    Xp = np.dot(X, p)  # (n_samples, 2)\n",
    "    s = softmax(Xp)  # (n_samples, 2), attention weights\n",
    "\n",
    "    # Compute attention-weighted sum of tokens\n",
    "    r = np.einsum('ij,ijk->ik', s, X)  # (n_samples, d)\n",
    "\n",
    "    # Compute model output\n",
    "    f_values = np.dot(r, v)  # Shape: (n_samples,)\n",
    "\n",
    "    return f_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benign Overfitting with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic loss\n",
    "def logistic_loss(z):\n",
    "    return np.log(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(X, y, v, p, beta):\n",
    "    \"\"\"\n",
    "    Compute gradients for v and p for a single iteration of GD.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: numpy array of shape (n_samples, 2, d), input data.\n",
    "    - y: numpy array of shape (n_samples,), labels (+1 or -1).\n",
    "    - v: numpy array of shape (d,), current vector v.\n",
    "    - p: numpy array of shape (d,), current vector p.\n",
    "    - beta: step size for gradient descent.\n",
    "    \n",
    "    Returns:\n",
    "    - Updated v and p after one iteration of gradient descent.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute predictions\n",
    "    predictions = single_head_attention(X, p, v)\n",
    "    \n",
    "    # Logistic loss gradient\n",
    "    sigmoid = 1 / (1 + np.exp(-y * predictions))  # (n_samples,)\n",
    "    grad_v = -np.dot((1 - sigmoid) * y, np.einsum('ij,ijk->ik', softmax(np.dot(X, p)), X)) / X.shape[0]  # (d,)\n",
    "    grad_p = np.zeros_like(p)  # Initialize gradient for p\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        attention_weights = softmax(np.dot(X[i], p))\n",
    "        grad_softmax = attention_weights[0] * (1 - attention_weights[0]) * X[i, 0] \\\n",
    "                     + attention_weights[1] * (1 - attention_weights[1]) * X[i, 1]\n",
    "        grad_p += (1 - sigmoid[i]) * y[i] * np.dot(v, grad_softmax)\n",
    "    \n",
    "    grad_p /= X.shape[0]\n",
    "    \n",
    "    # Update v and p using gradient descent\n",
    "    v -= beta * grad_v\n",
    "    p -= beta * grad_p\n",
    "    \n",
    "    return v, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, d, num_iterations, beta):\n",
    "    \"\"\"\n",
    "    Train the single-head attention model using gradient descent.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: numpy array of shape (n_samples, 2, d), input data.\n",
    "    - y: numpy array of shape (n_samples,), labels (+1 or -1).\n",
    "    - d: int, dimensionality of input data.\n",
    "    - num_iterations: int, number of GD iterations.\n",
    "    - beta: float, step size for gradient descent.\n",
    "    \n",
    "    Returns:\n",
    "    - v, p: Trained parameters.\n",
    "    \"\"\"\n",
    "    # Initialize parameters\n",
    "    v = np.zeros(d)\n",
    "    p = np.zeros(d)\n",
    "    \n",
    "    # Gradient descent\n",
    "    for t in range(num_iterations):\n",
    "        print('Iteration ', t)\n",
    "        v, p = compute_gradients(X, y, v, p, beta)\n",
    "    \n",
    "    return v, p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "np.random.seed(0)\n",
    "X, y  = generate_data(n_samples, d, rho, eta)\n",
    "num_iterations = 2\n",
    "beta = 0.01\n",
    "\n",
    "v, p = train_model(X, y, d, num_iterations, beta)\n",
    "print(\"Trained v:\", v)\n",
    "print(\"Trained p:\", p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
